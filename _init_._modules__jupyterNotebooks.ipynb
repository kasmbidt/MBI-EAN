{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://elin3t.medium.com/no-init-is-not-the-python-constructor-a36a71a234bc\n",
    "# https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Singleton.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main modules of extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-markdown-math\n",
      "  Downloading python_markdown_math-0.8-py3-none-any.whl (5.9 kB)\n",
      "Collecting Markdown>=3.0\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Installing collected packages: Markdown, python-markdown-math\n",
      "Successfully installed Markdown-3.3.4 python-markdown-math-0.8\n"
     ]
    }
   ],
   "source": [
    "#  install the jupyter_contrib_nbextensions notebook extensions\n",
    "# !pip install jupyter_contrib_nbextensions\n",
    "\n",
    "# Once installed, launch JupyterLab with: jupyter-lab\n",
    "# !pip install jupyterlab\n",
    "\n",
    "# !pip install python-markdown-math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask is a flexible library for parallel computing in Python.\n",
    "!pip install \"dask[complete]\" \n",
    "# !pip install \"modin[dask]\"\n",
    "# import modin.pandas as pd # to use dask in pandas\n",
    "\n",
    "# # Create a cluster to work with large datasets in dask\n",
    "# !pip install coiled\n",
    "\n",
    "# # Streaming processing\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections to DataBases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask-sql\n",
      "  Downloading dask_sql-0.3.6-py3-none-any.whl (19.3 MB)\n",
      "Requirement already satisfied: pygments in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask-sql) (2.7.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask-sql) (1.4.2)\n",
      "Requirement already satisfied: dask[dataframe]!=2021.3.0,>=2.19.0 in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask-sql) (2.30.0)\n",
      "Collecting fastapi>=0.61.1\n",
      "  Downloading fastapi-0.65.2-py3-none-any.whl (51 kB)\n",
      "Collecting jpype1>=1.0.2\n",
      "  Downloading JPype1-1.3.0-cp38-cp38-win_amd64.whl (362 kB)\n",
      "Requirement already satisfied: prompt-toolkit in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask-sql) (3.0.8)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask-sql) (1.1.3)\n",
      "Collecting uvicorn>=0.11.3\n",
      "  Downloading uvicorn-0.14.0-py3-none-any.whl (50 kB)\n",
      "Collecting tzlocal>=2.1\n",
      "  Using cached tzlocal-2.1-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask[dataframe]!=2021.3.0,>=2.19.0->dask-sql) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.13.0; extra == \"dataframe\" in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask[dataframe]!=2021.3.0,>=2.19.0->dask-sql) (1.19.2)\n",
      "Requirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask[dataframe]!=2021.3.0,>=2.19.0->dask-sql) (1.1.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask[dataframe]!=2021.3.0,>=2.19.0->dask-sql) (0.8.3)\n",
      "Requirement already satisfied: toolz>=0.8.2; extra == \"dataframe\" in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from dask[dataframe]!=2021.3.0,>=2.19.0->dask-sql) (0.11.1)\n",
      "Collecting starlette==0.14.2\n",
      "  Downloading starlette-0.14.2-py3-none-any.whl (60 kB)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from prompt-toolkit->dask-sql) (0.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->dask-sql) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->dask-sql) (2020.1)\n",
      "Requirement already satisfied: click>=7.* in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from uvicorn>=0.11.3->dask-sql) (7.1.2)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "Collecting asgiref>=3.3.4\n",
      "  Downloading asgiref-3.3.4-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: locket in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[dataframe]!=2021.3.0,>=2.19.0->dask-sql) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi>=0.61.1->dask-sql) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ksastoque\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->dask-sql) (1.15.0)\n",
      "Installing collected packages: starlette, pydantic, fastapi, jpype1, h11, asgiref, uvicorn, tzlocal, dask-sql\n",
      "Successfully installed asgiref-3.3.4 dask-sql-0.3.6 fastapi-0.65.2 h11-0.12.0 jpype1-1.3.0 pydantic-1.8.2 starlette-0.14.2 tzlocal-2.1 uvicorn-0.14.0\n"
     ]
    }
   ],
   "source": [
    "# # connection with MongoDB\n",
    "# !pip install pymongo\n",
    "\n",
    "# # Data migration Odo migrates data between different containers\n",
    "# !pip install odo\n",
    "\n",
    "# dask-sql is a distributed SQL query engine in Python. \n",
    "# It allows you to query and transform your data using a mixture \n",
    "# of common SQL operations and Python code and also scale up the \n",
    "# calculation easily if you need it.\n",
    "!pip install dask-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphics and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-0.9.0-py2.py3-none-any.whl (994 kB)\n",
      "Collecting shapely>=1.6\n",
      "  Downloading Shapely-1.7.1-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting fiona>=1.8\n",
      "  Downloading Fiona-1.8.20.tar.gz (1.3 MB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\ksastoque\\Anaconda3\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\ksastoque\\\\AppData\\\\Local\\\\Temp\\\\pip-install-0c073m5q\\\\fiona\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\ksastoque\\\\AppData\\\\Local\\\\Temp\\\\pip-install-0c073m5q\\\\fiona\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\ksastoque\\AppData\\Local\\Temp\\pip-pip-egg-info-vw234l4z'\n",
      "         cwd: C:\\Users\\ksastoque\\AppData\\Local\\Temp\\pip-install-0c073m5q\\fiona\\\n",
      "    Complete output (1 lines):\n",
      "    A GDAL API version must be specified. Provide a path to gdal-config using a GDAL_CONFIG environment variable or use a GDAL_VERSION environment variable.\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # An open-source, interactive data visualization library for Python\n",
    "# !pip install plotly -U\n",
    "# !pip install kaleido\n",
    "\n",
    "# # Interactive graphs adn dashboards\n",
    "# !pip install jupyter-dash\n",
    "# !pip install dash-bootstrap-components\n",
    "# !pip install jupyter-dash\n",
    "# !pip install dash-bootstrap-components\n",
    "\n",
    "# Altair is a declarative statistical visualization library for Python, based on Vega and Vega-Lite,\n",
    "# !pip install altair vega_datasets\n",
    "\n",
    "# # Colorful output in terminal. https://pypi.org/project/simple-colors/\n",
    "# !pip install simple-colors\n",
    "# !pip install --upgrade nbformat\n",
    "\n",
    "# # PyWaffle is an open source, MIT-licensed Python package for plotting waffle charts.\n",
    "# !pip install pywaffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting exrex\n",
      "  Using cached exrex-0.10.5.tar.gz (4.8 kB)\n",
      "Building wheels for collected packages: exrex\n",
      "  Building wheel for exrex (setup.py): started\n",
      "  Building wheel for exrex (setup.py): finished with status 'done'\n",
      "  Created wheel for exrex: filename=exrex-0.10.5-py3-none-any.whl size=9185 sha256=e5c70a8e269e235d18b28e7ac80922aef2ef05b623e2a8d96c0b4c052b94646b\n",
      "  Stored in directory: c:\\users\\ksastoque\\appdata\\local\\pip\\cache\\wheels\\98\\48\\67\\7a4d28cbc80e4e06885bf58324fbf362f0c7054ad8d33759bc\n",
      "Successfully built exrex\n",
      "Installing collected packages: exrex\n",
      "Successfully installed exrex-0.10.5\n"
     ]
    }
   ],
   "source": [
    "# # The module exports a function that takes an Unicode object (Python 2.x) or string (Python 3.x) \n",
    "# # and returns a string (that can be encoded to ASCII bytes in Python 3.x\n",
    "# !pip install unidecode\n",
    "\n",
    "# Package to efficiently use Levenshtein distance\n",
    "# !pip uninstall python-Levenshtein\n",
    "# !pip install python-Levenshtein\n",
    "\n",
    "# # Fuzzy string matching like a boss. It uses Levenshtein Distance to calculate the differences \n",
    "# # between sequences in a simple-to-use package.\n",
    "# !pip install fuzzywuzzy\n",
    "\n",
    "!pip install exrex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules to work with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting workdays\n",
      "  Using cached workdays-1.4.tar.gz (2.1 kB)\n",
      "Building wheels for collected packages: workdays\n",
      "  Building wheel for workdays (setup.py): started\n",
      "  Building wheel for workdays (setup.py): finished with status 'done'\n",
      "  Created wheel for workdays: filename=workdays-1.4-py3-none-any.whl size=2672 sha256=c566ee9a4ae16fe460bce7a30c240d8c652e8a5242a309a702981386060b59aa\n",
      "  Stored in directory: c:\\users\\ksastoque\\appdata\\local\\pip\\cache\\wheels\\3d\\b3\\96\\95652a26e39d06dabe3c7b0f0b7b4c4866ac256c439459a520\n",
      "Successfully built workdays\n",
      "Installing collected packages: workdays\n",
      "Successfully installed workdays-1.4\n"
     ]
    }
   ],
   "source": [
    "# Return working days between two dates exclude weekends and holidays.\n",
    "!pip install workdays\n",
    "!pip install python-nextetworkdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.1.0-py3-none-any.whl (112 kB)\n",
      "Collecting geographiclib<2,>=1.49\n",
      "  Downloading geographiclib-1.50-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.50 geopy-2.1.0\n"
     ]
    }
   ],
   "source": [
    "# geopy is a Python client for several popular geocoding web services.\n",
    "# geopy makes it easy for Python developers to locate the coordinates of addresses, cities, countries, \n",
    "# and landmarks across the globe using third-party geocoders and other data sources.\n",
    "!pip install geopy\n",
    "\n",
    "# # Geographic + Pandas\n",
    "# !pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # La principal biblioteca de código abierto para enseñarte a desarrollar y entrenar modelos de AA\n",
    "# # Aprendizaje Automático\n",
    "# !pip install --upgrade tensorflow\n",
    "\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retomar contextos válidos pre_procesados de una o más columnas \n",
    "# # para construir una nueva unificando los contextos válidos para \n",
    "\n",
    "# def unificar_contextos_validos(expresion, vars_contextos, var_to_create):\n",
    "#     \"\"\"\n",
    "#       Union de contextos validos de más de una columna en un solo diccionario\n",
    "#       Contextos validos = Procesamiento de nuevas expresiones regulares sobre contextos previamente extraidos\n",
    "#     \"\"\"\n",
    "#     var_to_create = {\n",
    "#                      \"nid\": \"\",\n",
    "#                      \"contextos\": [],\n",
    "#                      \"resultadoFinal\": [],\n",
    "#                      \"label\": []\n",
    "#                     } \n",
    "    \n",
    "# #     doc_validos = []\n",
    "# #     ctx_pagina_validos = []\n",
    "# #     parrafos_validos = []\n",
    "    \n",
    "#     for var_contextos in vars_contextos:  # recorreremos todas las variables\n",
    "#         if var_contextos is None:\n",
    "#             var_to_create[\"contextos\"] = []\n",
    "        \n",
    "#             return var_to_create\n",
    "        \n",
    "#         doc_validos = []\n",
    "#         for doc in var_contextos[\"contextos\"]: # contextos de cada variable\n",
    "\n",
    "#             ctx_pagina_validos = []\n",
    "#             for ctx_pagina in doc[\"contextos\"]: # cada contexto en especifico\n",
    "\n",
    "#                 parrafos_validos = []\n",
    "#                 for parrafo in ctx_pagina[\"parrafos\"]: # cada parrafo en especifico\n",
    "                    \n",
    "#                     palabra_clave = re.findall(expresion, parrafo[\"texto\"])\n",
    "                    \n",
    "#                     if len(palabra_clave) > 0:\n",
    "#                         parrafo[\"palabra_clave\"] += f'|{palabra_clave[0]}'\n",
    "\n",
    "#                         parrafos_validos.append(parrafo)\n",
    "                        \n",
    "                        \n",
    "\n",
    "#                 if parrafos_validos:\n",
    "\n",
    "#                     ctx_pagina_validos.append({\"pagina\":ctx_pagina[\"pagina\"], \"parrafos\":parrafos_validos})\n",
    "            \n",
    "#             if ctx_pagina_validos:\n",
    "                \n",
    "#                 doc_validos.append({\"documento\": doc[\"documento\"], \"contextos\": ctx_pagina_validos})\n",
    "            \n",
    "#         var_to_create[\"contextos\"] += doc_validos\n",
    "    \n",
    "#     return var_to_create\n",
    "  \n",
    "# unificar_contextos_validos_udf = udf(unificar_contextos_validos, struct_procesar_expresion_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assign(D, n):\n",
    "#     \"\"\"\n",
    "#     Assigns 'n' patients to 'len(D.values())' EPS.\n",
    "#     n = daily goal (or adaptation percentage if is indicated < 1)\n",
    "\n",
    "#     Algorithm:\n",
    "#     input: [2,3,1], n = 4\n",
    "\n",
    "#     * Subtracts 1 to each non-zero element of 'values' until n->0 or\n",
    "#     there isn't more than 'n' non-zero elements of 'values'.\n",
    "#     [1,2,0], n = 1\n",
    "\n",
    "#     * Assigns patients to the smallest non-zero element of 'values' until n->0\n",
    "#     [0,2,0], n = 0\n",
    "\n",
    "#     Input: 'values' Dict, 'n' Int\n",
    "#     Output: 'new_assign', 'residuals(to reassign)', 'new_n(patients left to reach the goal (n))'\n",
    "#     \"\"\"\n",
    "#     d = D\n",
    "#     keys = d.keys()\n",
    "#     values = np.asarray(list(d.values()))\n",
    "\n",
    "#     # Subtracts 1 to each non-zero element of 'values' until n->0 or\n",
    "#     # there isn't more than 'n' non-zero elements of 'values' (i.e [1,2,0], 1).\n",
    "#     while n > 0 and n >= len(values[values != 0]) and any(values):\n",
    "#         nAffected = len(values[values != 0])\n",
    "#         values[values != 0] -= 1\n",
    "#         n -= nAffected\n",
    "\n",
    "#     # Assigns patients to the smallest non-zero element of 'values'\n",
    "#     # until n->0 (i.e [0,2,0],0)\n",
    "#     if any(values):\n",
    "\n",
    "#         while n > 0:\n",
    "\n",
    "#             minV = values[values != 0].min()\n",
    "#             argminV = np.where(values == minV)[0][0]  # First index of MinV\n",
    "\n",
    "#             if minV <= n:\n",
    "#                 values[argminV] = 0\n",
    "#                 n -= minV\n",
    "#                 print(values, n)\n",
    "#             else:\n",
    "#                 values[argminV] -= n\n",
    "#                 n = 0\n",
    "\n",
    "#     residual = {k:v for k, v in zip(*[keys, values])}\n",
    "#     D_new = dict(Counter(D) - Counter(residual))\n",
    "    \n",
    "#     return D_new, residual, n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
